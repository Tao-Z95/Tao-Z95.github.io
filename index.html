<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-KG-task01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/01/11/KG-task01/" class="article-date">
  <time datetime="2021-01-11T13:23:32.000Z" itemprop="datePublished">2021-01-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/01/11/KG-task01/">KG_task01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="知识图谱的定义"><a href="#知识图谱的定义" class="headerlink" title="知识图谱的定义"></a>知识图谱的定义</h1><p>学术角度：语义网络(Semantic Network)的知识库</p>
<p>应用角度：多关系图(Multi-relational Graph) —-包含多种类型节点和多种类型边</p>
<h2 id="知识图谱中的重要概念：Schema"><a href="#知识图谱中的重要概念：Schema" class="headerlink" title="知识图谱中的重要概念：Schema"></a>知识图谱中的重要概念：Schema</h2><p>用于限定待加入知识图谱数据的格式。</p>
<p>DataType:限定知识图谱节点值的类型</p>
<p>Thing:限定节点的类型及属性</p>
<p><img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20210111213837054.png" alt="image-20210111213837054"></p>
<p>举例说明：基于上图Schema构建的知识图谱中仅可含作品、地方组织、人物；其中作品的属性为电影与音乐、地方组织的属性为当地的商业（eg：饭店、俱乐部等）、人物的属性为歌手</p>
<h3 id="知识图谱的数据来源"><a href="#知识图谱的数据来源" class="headerlink" title="知识图谱的数据来源"></a>知识图谱的数据来源</h3><ul>
<li>  结构化数据：数据库，仅需简单预处理</li>
<li>  非结构化数据：网页，需要借助自然语言处理等技术提取结构化信息</li>
</ul>
<p>例：下图表示从英文文本(非结构化数据)中抽取的实体和关系。</p>
<p><img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20210111214408276.png" alt="image-20210111214408276"></p>
<h2 id="构建知识图谱所涉及的技术"><a href="#构建知识图谱所涉及的技术" class="headerlink" title="构建知识图谱所涉及的技术"></a>构建知识图谱所涉及的技术</h2><h3 id="命名实体识别（Name-Entity-Recognition）"><a href="#命名实体识别（Name-Entity-Recognition）" class="headerlink" title="命名实体识别（Name Entity Recognition）"></a>命名实体识别（Name Entity Recognition）</h3><p>目标：从文本里提取出实体并对每个实体做分类/打标签</p>
<p>举例说明：提取出实体“NYC”，并标记实体类型为 “Location”；我们也可以从中提取出“Virgil’s BBQ”，并标记实体类型为“Restarant”</p>
<h3 id="关系抽取（Relation-Extraction）"><a href="#关系抽取（Relation-Extraction）" class="headerlink" title="关系抽取（Relation Extraction）"></a>关系抽取（Relation Extraction）</h3><p>目标：把实体间的关系从文本中提取出来</p>
<p>举例说明：比如实体“hotel”和“Hilton property”之间的关系为“in”；“hotel”和“Time Square”的关系为“near”等等</p>
<h3 id="实体统一（Entity-Resolution）"><a href="#实体统一（Entity-Resolution）" class="headerlink" title="实体统一（Entity Resolution）"></a>实体统一（Entity Resolution）</h3><p>目的：有些实体写法上不一样，但其实是指向同一个实体，对这些实体进行合并</p>
<p>举例说明：比如“NYC”和“New York”表面上是不同的字符串，但其实指的都是纽约这个城市，需要合并</p>
<h3 id="指代消解（Disambiguation）"><a href="#指代消解（Disambiguation）" class="headerlink" title="指代消解（Disambiguation）"></a>指代消解（Disambiguation）</h3><p>目的：还原代词指向的实体</p>
<p>举例说明：文本中出现的“it”, “he”, “she”这些词到底指向哪个实体，比如在本文里两个被标记出来的“it”都指向“hotel”这个实体</p>
<h2 id="知识图谱的存储"><a href="#知识图谱的存储" class="headerlink" title="知识图谱的存储"></a>知识图谱的存储</h2><ul>
<li><p>基于RDF的存储</p>
<p>存储形式：以三元组形式且不包含属性信息</p>
<p>特点：易于发布和共享</p>
<p>例：Jena</p>
</li>
</ul>
<ul>
<li><p>基于图数据库的存储</p>
<p>  存储形式：以属性图为基本的表示形式，所以实体和关系可以包含属性</p>
<p>  特点：高效的图查询和搜索</p>
<p>  例：Neo4j、OrientDB、JanusGraph</p>
</li>
</ul>
<p><img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20210111215749937.png" alt="image-20210111215749937"></p>
<h1 id="Neo4j介绍与安装"><a href="#Neo4j介绍与安装" class="headerlink" title="Neo4j介绍与安装"></a>Neo4j介绍与安装</h1><h2 id="Neo4j的安装-Mac"><a href="#Neo4j的安装-Mac" class="headerlink" title=" Neo4j的安装(Mac)"></a> Neo4j的安装(Mac)</h2><p>参照博客<a target="_blank" rel="noopener" href="https://blog.csdn.net/huacha__/article/details/81123410">https://blog.csdn.net/huacha__/article/details/81123410</a></p>
<p>Neo4j  3.x   对应  jdk 8.x</p>
<p>Neo4j  4.x   对应 jdk 11.x</p>
<p> 登陆本地库时可能会出现下图的问题</p>
<p><img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20210111235815808.png" alt="image-20210111235815808"></p>
<p>输入  用户名：neo4j    密码：neo4j    即可</p>
<h2 id="Web界面使用Neo4j"><a href="#Web界面使用Neo4j" class="headerlink" title="Web界面使用Neo4j"></a>Web界面使用Neo4j</h2><p>只有一些简单的查询操作才在web界面中进行，一般还是使用Python等的driver在程序中实现</p>
<h3 id="查询语言：Cypher-CQL"><a href="#查询语言：Cypher-CQL" class="headerlink" title="查询语言：Cypher (CQL)"></a>查询语言：Cypher (CQL)</h3><ul>
<li>  介绍：声明式图形查询语言，使得用户不必编写图结构的遍历代码，对图形数据实现高效查询</li>
<li>  目的：类似SQL，可在数据库上实现点对点模式(ad-hoc)查询</li>
<li>  功能：创建、更新、删除节点和关系</li>
</ul>
<p>Tips:使用web界面创建节点后，要显示所有节点的大图的话，点击左侧Node Labels下的图标即可</p>
<h2 id="用Python操作Neo4j"><a href="#用Python操作Neo4j" class="headerlink" title="用Python操作Neo4j"></a>用Python操作Neo4j</h2><h3 id="neo4j模块-执行CQL语句"><a href="#neo4j模块-执行CQL语句" class="headerlink" title="neo4j模块:执行CQL语句"></a>neo4j模块:执行CQL语句</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step 1：导入 Neo4j 驱动包</span></span><br><span class="line">  <span class="keyword">from</span> neo4j <span class="keyword">import</span> GraphDatabase</span><br><span class="line">  <span class="comment"># step 2：连接 Neo4j 图数据库</span></span><br><span class="line">  driver = GraphDatabase.driver(<span class="string">&quot;bolt://localhost:7687&quot;</span>, auth=(<span class="string">&quot;neo4j&quot;</span>, <span class="string">&quot;password&quot;</span>))</span><br><span class="line">  <span class="comment"># 添加 关系 函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add_friend</span>(<span class="params">tx, name, friend_name</span>):</span></span><br><span class="line">      tx.run(<span class="string">&quot;MERGE (a:Person &#123;name: $name&#125;) &quot;</span></span><br><span class="line">            <span class="string">&quot;MERGE (a)-[:KNOWS]-&gt;(friend:Person &#123;name: $friend_name&#125;)&quot;</span>,</span><br><span class="line">            name=name, friend_name=friend_name)</span><br><span class="line">  <span class="comment"># 定义 关系函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print_friends</span>(<span class="params">tx, name</span>):</span></span><br><span class="line">      <span class="keyword">for</span> record <span class="keyword">in</span> tx.run(<span class="string">&quot;MATCH (a:Person)-[:KNOWS]-&gt;(friend) WHERE a.name = $name &quot;</span></span><br><span class="line">                          <span class="string">&quot;RETURN friend.name ORDER BY friend.name&quot;</span>, name=name):</span><br><span class="line">          print(record[<span class="string">&quot;friend.name&quot;</span>])</span><br><span class="line">  <span class="comment"># step 3：运行</span></span><br><span class="line">  <span class="keyword">with</span> driver.session() <span class="keyword">as</span> session:</span><br><span class="line">      session.write_transaction(add_friend, <span class="string">&quot;Arthur&quot;</span>, <span class="string">&quot;Guinevere&quot;</span>)</span><br><span class="line">      session.write_transaction(add_friend, <span class="string">&quot;Arthur&quot;</span>, <span class="string">&quot;Lancelot&quot;</span>)</span><br><span class="line">      session.write_transaction(add_friend, <span class="string">&quot;Arthur&quot;</span>, <span class="string">&quot;Merlin&quot;</span>)</span><br><span class="line">      session.read_transaction(print_friends, <span class="string">&quot;Arthur&quot;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="py2neo模块：通过操作python变量，达到操作neo4j的目的"><a href="#py2neo模块：通过操作python变量，达到操作neo4j的目的" class="headerlink" title="py2neo模块：通过操作python变量，达到操作neo4j的目的"></a>py2neo模块：通过操作python变量，达到操作neo4j的目的</h3><p>更符合Python的习惯，不需要写CQL</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># step 1：导入 Neo4j 驱动包</span></span><br><span class="line">  <span class="keyword">from</span> neo4j <span class="keyword">import</span> GraphDatabase</span><br><span class="line">  <span class="comment"># step 2：连接 Neo4j 图数据库</span></span><br><span class="line">  driver = GraphDatabase.driver(<span class="string">&quot;bolt://localhost:7687&quot;</span>, auth=(<span class="string">&quot;neo4j&quot;</span>, <span class="string">&quot;password&quot;</span>))</span><br><span class="line">  <span class="comment"># 添加 关系 函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add_friend</span>(<span class="params">tx, name, friend_name</span>):</span></span><br><span class="line">      tx.run(<span class="string">&quot;MERGE (a:Person &#123;name: $name&#125;) &quot;</span></span><br><span class="line">            <span class="string">&quot;MERGE (a)-[:KNOWS]-&gt;(friend:Person &#123;name: $friend_name&#125;)&quot;</span>,</span><br><span class="line">            name=name, friend_name=friend_name)</span><br><span class="line">  <span class="comment"># 定义 关系函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">print_friends</span>(<span class="params">tx, name</span>):</span></span><br><span class="line">      <span class="keyword">for</span> record <span class="keyword">in</span> tx.run(<span class="string">&quot;MATCH (a:Person)-[:KNOWS]-&gt;(friend) WHERE a.name = $name &quot;</span></span><br><span class="line">                          <span class="string">&quot;RETURN friend.name ORDER BY friend.name&quot;</span>, name=name):</span><br><span class="line">          print(record[<span class="string">&quot;friend.name&quot;</span>])</span><br><span class="line">  <span class="comment"># step 3：运行</span></span><br><span class="line">  <span class="keyword">with</span> driver.session() <span class="keyword">as</span> session:</span><br><span class="line">      session.write_transaction(add_friend, <span class="string">&quot;Arthur&quot;</span>, <span class="string">&quot;Guinevere&quot;</span>)</span><br><span class="line">      session.write_transaction(add_friend, <span class="string">&quot;Arthur&quot;</span>, <span class="string">&quot;Lancelot&quot;</span>)</span><br><span class="line">      session.write_transaction(add_friend, <span class="string">&quot;Arthur&quot;</span>, <span class="string">&quot;Merlin&quot;</span>)</span><br><span class="line">      session.read_transaction(print_friends, <span class="string">&quot;Arthur&quot;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="通过csv"><a href="#通过csv" class="headerlink" title="通过csv"></a>通过csv</h3><p>需要两个csv文件，分为node.csv和relation.csv</p>
<p>1、两个文件nodes.csv ，relas.csv放在</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">neo4j安装绝对路径&#x2F;import</span><br></pre></td></tr></table></figure>

<p>2、导入到图数据库mygraph.db</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">neo4j bin&#x2F;neo4j-admin import --nodes&#x3D;&#x2F;var&#x2F;lib&#x2F;neo4j&#x2F;import&#x2F;nodes.csv --relationships&#x3D;&#x2F;var&#x2F;lib&#x2F;neo4j&#x2F;import&#x2F;relas.csv   --delimiter&#x3D;^ --database&#x3D;xinfang*.db</span><br></pre></td></tr></table></figure>

<p>delimiter=^ 指的是csv的分隔符</p>
<p>3、指定neo4j使用哪个数据库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">修改 &#x2F;root&#x2F;neo4j&#x2F;conf&#x2F;neo4j.conf 文件中的 dbms.default_database&#x3D;mygraph.db</span><br></pre></td></tr></table></figure>

<p>4、重启neo4j就可以看到数据已经导入成功了</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/01/11/KG-task01/" data-id="ckjtdrm9500001b1034p9g0o0" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Heterogeneous Graph Neural Networks for Extractive Document Summarization" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/12/02/Heterogeneous%20Graph%20Neural%20Networks%20for%20Extractive%20Document%20Summarization/" class="article-date">
  <time datetime="2020-12-02T08:54:50.074Z" itemprop="datePublished">2020-12-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>title:Heterogeneous Graph Neural Networks for Extractive Document Summarization<br>Date:2020-11-25 14:43:12<br>tags:note</p>
<hr>
<p><img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201202165457002.png" alt="image-20201202165457002"></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本篇文章提出了一种用异质图来做摘要抽取的方式。</p>
<p>之前的用图来做summarization抽取的工作：</p>
<ul>
<li><p>节点：把句子作为图中的节点</p>
</li>
<li><p>  边：通过句子间的统计信息或者语言学上的信息来得到图的边/全连接图，这样构建出来的图是同质的。</p>
</li>
</ul>
<p><strong>Motivation</strong>:加入其它不同细粒度的语义节点，以丰富句子间的关系，更好的得到句子的representation</p>
<p>而本篇论文说的是为了简化起见，提出了只把单词节点作为语义单元，单词节点作为连接句子间的中介，提供了更加丰富的cross-sentence信息。(文章也提到可以加入entity，concept等语义单元)</p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201204184134515.png" alt="image-20201204184134515" style="zoom: 33%;" />

<p>模型由三部分组成：</p>
<h3 id="Graph-Initializers"><a href="#Graph-Initializers" class="headerlink" title="Graph Initializers"></a>Graph Initializers</h3><ul>
<li>  Sentence node：对于每一个句子先用不同kernal大小的CNN提取每个句子的n-gram信息,得到句子的local feaure $l_j$</li>
</ul>
<p>​                               再用双向LSTM得到句子的global feature   $g_j$</p>
<p>​                               最后得到sentence node feature  $X_{s_{j}}=\left[l_{j} ; g_{j}\right]$</p>
<ul>
<li>  Edge feature:  句子节点和词节点之间边的权重，由TF-IDF值确定，TF是单词在句子中出现的次数，IDF考量的是单词在文档句子中的出现次数。</li>
</ul>
<h3 id="Heterogeneous-Graph-Layer"><a href="#Heterogeneous-Graph-Layer" class="headerlink" title="Heterogeneous Graph Layer"></a>Heterogeneous Graph Layer</h3><ul>
<li>  用GAT来更新图中节点的信息</li>
</ul>
<div>
 $$
z_{i j}=\text { LeakyReLU }\left(\mathbf{W}_{a}\left[\mathbf{W}_{q} \boldsymbol{h}_{i} ; \mathbf{W}_{k} \boldsymbol{h}_{j}\right]\right)
$$
</div>

<p>$$<br>z_{i j}=\text { LeakyReLU }\left(\mathbf{W}<em>{a}\left[\mathbf{W}</em>{q} \boldsymbol{h}<em>{i} ; \mathbf{W}</em>{k} \boldsymbol{h}_{j}\right]\right)<br>$$</p>
<p>$$<br>\alpha_{i j}=\frac{\exp \left(z_{i j}\right)}{\sum_{l \in \mathcal{N}<em>{i}} \exp \left(z</em>{i l}\right)}<br>$$</p>
<p>$$<br>\boldsymbol{u}<em>{i}=\sigma\left(\sum</em>{j \in \mathcal{N}<em>{i}} \alpha</em>{i j} \mathbf{W}<em>{v} \boldsymbol{h}</em>{j}\right)<br>$$</p>
<ul>
<li>  引入了多头注意力</li>
</ul>
<p>$$<br>\boldsymbol{u}<em>{i}=|</em>{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}<em>{i}} \alpha</em>{i j}^{k} \mathbf{W}^{k} \boldsymbol{h}_{i}\right)<br>$$</p>
<ul>
<li>  引入了residual connection</li>
</ul>
<p>$$<br>\boldsymbol{h}<em>{i}^{\prime}=\boldsymbol{u}</em>{i}+\boldsymbol{h}_{i}<br>$$</p>
<ul>
<li>  在GAT中加入了边的权重$e_{i j}$考虑了</li>
</ul>
<p>$$<br>z_{i j}=\text { LeakyReLU }\left(\mathbf{W}<em>{a}\left[\mathbf{W}</em>{q} \boldsymbol{h}<em>{i} ; \mathbf{W}</em>{k} \boldsymbol{h}<em>{j} ; \boldsymbol{e}</em>{i j}\right]\right)<br>$$</p>
<p>​                               其中，$\boldsymbol{e}<em>{i j} \in\mathbb{R}^{m n \times d</em>{e}}$   ， 把原来属于标量的权重先映射到向量空间中，对边的权重做一个embedding</p>
<ul>
<li><p>参考了Transformer,即在节点中加入了单词顺序的postion encoding </p>
<p>  所以在经过GAT之后，还将节点经过了一层position-wise feed-forward (FFN) layer</p>
</li>
</ul>
<h4 id="Iterative-updating"><a href="#Iterative-updating" class="headerlink" title="Iterative updating"></a>Iterative updating</h4><img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201208000050232.png" alt="image-20201208000050232" style="zoom: 50%;" />

<p>第一次迭代时，只更新sentence节点，表示成矩形形式：<br>$$<br>\mathbf{U}<em>{s \leftarrow w}^{1}=\operatorname{GAT}\left(\mathbf{H}</em>{s}^{0}, \mathbf{H}<em>{w}^{0}, \mathbf{H}</em>{w}^{0}\right)<br>$$</p>
<p>$$<br>\mathbf{H}<em>{s}^{1}=\mathrm{FFN}\left(\mathbf{U}</em>{s \leftarrow w}^{1}+\mathbf{H}_{s}^{0}\right)<br>$$</p>
<p>以后的每次迭代都分为两步，先更新句子节点的表征，再更新单词节点的表征：<br>$$<br>\begin{aligned}<br>\mathbf{U}<em>{w \leftarrow s}^{t+1} &amp;=\operatorname{GAT}\left(\mathbf{H}</em>{w}^{t}, \mathbf{H}<em>{s}^{t}, \mathbf{H}</em>{s}^{t}\right) \<br>\mathbf{H}<em>{w}^{t+1} &amp;=\mathrm{FFN}\left(\mathbf{U}</em>{w \leftarrow s}^{t+1}+\mathbf{H}<em>{w}^{t}\right) \<br>\mathbf{U}</em>{s \leftarrow w}^{t+1} &amp;=\operatorname{GAT}\left(\mathbf{H}<em>{s}^{t}, \mathbf{H}</em>{w}^{t+1}, \mathbf{H}<em>{w}^{t+1}\right) \<br>\mathbf{H}</em>{s}^{t+1} &amp;=\mathrm{FFN}\left(\mathbf{U}<em>{s \leftarrow w}^{t+1}+\mathbf{H}</em>{s}^{t}\right)<br>\end{aligned}<br>$$</p>
<h2 id="Sentence-Selector"><a href="#Sentence-Selector" class="headerlink" title="Sentence Selector"></a>Sentence Selector</h2><p>文章用的是抽取方式生成summarization，所以在对sentence 节点encode之后，对所有句子进行排序，从前到后选择组成摘要的句子，同时加入了Trigram blocking机制，如果当前句子和已经被选为组成摘要的句子trigram有重合，那么直接跳过这个句子</p>
<h2 id="Multi-document-Summarization"><a href="#Multi-document-Summarization" class="headerlink" title="Multi-document Summarization"></a>Multi-document Summarization</h2><img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201208003725217.png" alt="image-20201208003725217" style="zoom:50%;" />

<p>这篇论文还将提出的框架套用到了多文档摘要的生成上，具体做法就是加入document节点，还是通过单词节点作为中介语义单元来构建文档与文档之间，文档与句子之间的关系。</p>
<ul>
<li>  document encoding: 对句子节点进行均值池化得到</li>
<li>  更新：把document 节点当做特殊的句子节点</li>
<li>  Sentence selection：把句子节点和文档节点的表征拼接起来，作为句子的表征</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>模型在CNN/DailyMail上的表现：</p>
<img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201208084255133.png" alt="image-20201208084255133" style="zoom:50%;" />

<p>与建立句子之间全连接的模型Ext相比，有了提升。说明这种异质图能更有效的建立句子间的联系</p>
<p>​                </p>
<p>模型在NYT50的表现如上图。这上面用了Tri-Blocking效果反而比不用更差，应该是和数据集构成摘要的特点有关。</p>
<img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201208090729684.png" alt="image-20201208090729684" style="zoom:50%;" />



<p>多文档摘要生成方面的表现：</p>
<img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201208091014341.png" alt="image-20201208091014341" style="zoom:50%;" />



<h2 id="Ablation-studies"><a href="#Ablation-studies" class="headerlink" title="Ablation studies"></a>Ablation studies</h2><img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201208091236239.png" alt="image-20201208091236239" style="zoom:50%;" />


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/12/02/Heterogeneous%20Graph%20Neural%20Networks%20for%20Extractive%20Document%20Summarization/" data-id="ckjsi4f9z0000qz10bov9ce7m" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Topic-Aware Neural Keyphrase Generation for Social Media Language" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/11/16/Topic-Aware%20Neural%20Keyphrase%20Generation%20for%20Social%20Media%20Language/" class="article-date">
  <time datetime="2020-11-16T01:57:00.074Z" itemprop="datePublished">2020-11-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>title:Topic-Aware Neural Keyphrase Generation for Social Media Language<br>date:2020-12-27 14:43:12<br>tags:note</p>
<hr>
<p><img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201116095902456.png" alt="image-20201116095902456"></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>现有的seq2seq模型对于处理经过精心编辑过的文章(科学文章)很有效</p>
<p>社交媒体中的文本：不正式，口语化</p>
<p>data sparsity：文本中的噪声比较大，而有用信息比较少</p>
<p>本文提出的关键短语生成模型主要应用于处理这类社交媒体文本。</p>
<p><strong>intuition</strong>： topic words — naturally indicative of keyphrases</p>
<h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul>
<li><p> Leverag latent topic to enrich useful features</p>
<p>  提出了 一种新的基于主题感知的神经关键词生成模型。利用corpus-level latent topic来解决data sparsity问题，且这种方式不需要外部数据</p>
</li>
<li><p>taking advantage of the recent advance of neural topic model</p>
<p>  将神经主题模型引入端到端的关键短语生成中，并且一起训练</p>
</li>
<li><p>  experiment on three newly constructed so- cial media datasets</p>
</li>
</ul>
<p>​       在Twitter、 StackExchange、Weibo三个数据集上进行了实验，结果都比其他没有用到潜在主题的模型效果好。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><blockquote>
<p>  [Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, and Yu Chi. 2017. Deep keyphrase generation. In <em>Proceedings of Associa- tion for Computational Linguistics</em>.]</p>
</blockquote>
<blockquote>
<p>  [Jichuan Zeng, Jing Li, Yan Song, Cuiyun Gao, Michael R. Lyu, and Irwin King. 2018. Topic mem- ory networks for short text classification. In <em>Pro- ceedings of Empirical Methods in Natural Language Processing</em>.]</p>
</blockquote>
<p>本文提出的方法是结合了主题感知的关键短语生成，其模型框架如下图所示：</p>
<img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201116160207666.png" alt="image-20201116160207666" style="zoom: 50%;" />



<p>其中绿色的部分表示主题模型（topic model），上面的两部分整体上表示生成模型（generation model）<br>所要处理的语料库记为$\left{\mathbf{x}<em>{1}, \mathbf{x}</em>{2},\ldots,\mathbf{x}_{|C|}\right}$ ，$|C|$表示语料库中推文的数量，我们将每篇推文$\mathbf{x}$都处理成两种表示：</p>
<ul>
<li><p>一种是基于词袋模型的向量表示$\mathbf{x}_{bow}$，作为神经主题模型的输入。</p>
<p>  ​      其中，$\mathbf{x}_{bow}$是$V$维向量，$V$表示词汇表大小</p>
</li>
<li><p>  一种是词典索引序列向量的表示$\mathbf{x}_{seq}$,作为关键短语生成模型的输入</p>
</li>
</ul>
<h2 id="Neural-Topic-Model"><a href="#Neural-Topic-Model" class="headerlink" title="Neural Topic Model"></a>Neural Topic Model</h2><p>NTM整体上基于变分自编码器(Variational Auto-Encoder,VAE)的架构，</p>
<p>本文中的Neural topic Model就是用神经网络的方式来拟合文章的主题分布，它的任务就是对$\mathbf{x}<em>{bow}$进行重构得到$\mathbf{x}^{’}</em>{bow}，优化目标就是让两者尽可能接近。把中间过程得到的隐表示$\mathbf{z}$(表示$\mathbf{x}$的主题)参与生成模型Decoder的生成过程。</p>
<h3 id="BoW-Encoder"><a href="#BoW-Encoder" class="headerlink" title="BoW Encoder"></a>BoW Encoder</h3><p>这里BoW 编码器是用来生成这篇文章的主题的隐表示$\mathbf{z}$，其中 $\mathbf{z} \in \mathbb{R}^{K}$，$K$表示主题的数量，假设文章的主题的隐表示是连续的，且服从正态分布的,那只要根据$\mu$和$\sigma$就能生成服从正态分布的随机数$\mathbf{z}$。<br>$$<br>\mathbf{z} \sim \mathcal{N}\left(\mu, \sigma^{\mathbf{2}}\right)<br>$$<br>那么encoder的作用就是确定正态分布的两个参数$\mu$和$\sigma$。<br>$$<br>\mu=f_{\mu}\left(f_{e}\left(\mathbf{x}<em>{b o w}\right)\right), \log \sigma=f</em>{\sigma}\left(f_{e}\left(\mathbf{x}_{b o w}\right)\right)<br>$$</p>
<p>公式中的$f_{*}(·)$是以ReLU作为激活函数的感知机层。</p>
<blockquote>
<p>  [Jichuan Zeng, Jing Li, Yan Song, Cuiyun Gao, Michael R. Lyu, and Irwin King. 2018. Topic memory networks for short text classification. In <em>Pro- ceedings of Empirical Methods in Natural Language Processing</em>.]</p>
</blockquote>
<h3 id="BoW-Decoder"><a href="#BoW-Decoder" class="headerlink" title="BoW Decoder"></a>BoW Decoder</h3><p>对于语料库中的每篇推文，都有一个$K$维的topic mixture的概率分布θ：</p>
<p>$$<br>\theta=\operatorname{softmax}\left(f_{\theta}(\mathbf{z})\right)<br>$$<br>对于文章中的每个词$w\in\mathbf{x}$<br>$$<br>w \sim \operatorname{softmax}\left(f_{\phi}(\theta)\right)<br>$$<br>其中$f_{*}(·)$也是以ReLU作为激活函数的感知机层，那么$f_{\phi}(·)$的参数矩阵作为topic-word的分布，即每个topic对应的词的分布：<br>$$<br>\left(\phi_{1}, \phi_{2}, \ldots, \phi_{K}\right)<br>$$</p>
<h2 id="Neural-Keyphrase-Generation-Model"><a href="#Neural-Keyphrase-Generation-Model" class="headerlink" title="Neural Keyphrase Generation Model"></a>Neural Keyphrase Generation Model</h2><p>本文的Keyphrase Generation Model还是一个seq2seq的的模型，但是在decoder的输入端加入了文章的latent topic representation。</p>
<h3 id="Sequence-Encoder"><a href="#Sequence-Encoder" class="headerlink" title="Sequence Encoder"></a>Sequence Encoder</h3><p>采用了Bi-GRU来进行encode,<br>$$<br>\overrightarrow{\mathbf{h}}<em>{i}=f</em>{G R U}\left(\nu_{i}, \mathbf{h}_{i-1}\right)<br>$$</p>
<p>$$<br>\overleftarrow{\mathbf{h}}<em>{i}=f</em>{G R U}\left(\nu_{i}, \mathbf{h}_{i-1}\right)<br>$$</p>
<p>其中,</p>
<p>​                  $\nu_{i}$表示输入序列中词$w_i$的embedding</p>
<p>最后，</p>
<p>​                   得到${\mathbf{h}}<em>{i}=[\overrightarrow{\mathbf{h}}</em>{i};\overleftarrow{\mathbf{h}}_{i}]$  作为$w_i$的hidden state </p>
<p>​                   矩阵$\mathbf{M}=\left\langle\mathbf{h}<em>{1}, \mathbf{h}</em>{2}, \ldots, \mathbf{h}_{|\mathbf{x}|}\right\rangle$  用于做attention</p>
<h3 id="Topic-Aware-Sequence-Decoder"><a href="#Topic-Aware-Sequence-Decoder" class="headerlink" title="Topic-Aware Sequence Decoder"></a>Topic-Aware Sequence Decoder</h3><p>根据Sequence Encoder和Neural topic model的输出，生成每个关键短语的概率可表示为：<br>$$<br>\operatorname{Pr}(\mathbf{y} \mid \mathbf{x})=\prod_{j=1}^{|\mathbf{y}|} \operatorname{Pr}\left(y_{j} \mid \mathbf{y}_{&lt;j}, \mathbf{M}, \theta\right)<br>$$</p>
<ul>
<li>本文的sequence decoder采用了一个单向GRU，第$j$个时间步的hidden state：</li>
</ul>
<p>$$<br>\mathbf{s}<em>{j}=f</em>{G R U}\left(\left[\mathbf{u}<em>{j} ; \theta\right], \mathbf{s}</em>{j-1}\right)<br>$$</p>
<p>其中,</p>
<p>​                         $\mathbf{u}_{j}$是decoder第j步输出的embedding</p>
<ul>
<li><p>本文的decoder还采用了attention机制</p>
<p>   attention的权重：<br>  $$<br>  \alpha_{i j}=\frac{\exp \left(f_{\alpha}\left(\mathbf{h}<em>{i}, \mathbf{s}</em>{j}, \theta\right)\right)}{\sum_{i^{\prime}=1}^{|\mathbf{x}|} \exp \left(f_{\alpha}\left(\mathbf{h}<em>{i^{\prime}}, \mathbf{s}</em>{j}, \theta\right)\right)}<br>  $$</p>
</li>
</ul>
<p>​          其中，</p>
<p>​                                       $f_{\alpha}\left(\mathbf{h}<em>{i}, \mathbf{s}</em>{j}, \theta\right)=\mathbf{v}<em>{\alpha}^{T} \tanh \left(\mathbf{W}</em>{\alpha}\left[\mathbf{h}<em>{i} ; \mathbf{s}</em>{j} ; \theta\right]+\mathbf{b}_{\alpha}\right)$</p>
<p>​           得到topic sensitive context vector $\mathbf{c}_{j}$ ：</p>
<p>​                                                  $\mathbf{c}<em>{j}=\sum</em>{i=1}^{|\mathbf{x}|} \alpha_{i j} \mathbf{h}_{i}$</p>
<p>​           </p>
<p>​        最终得到第j个词在词汇表上的生成概率：</p>
<p>​      $p_{g e n}=\operatorname{softmax}\left(\mathbf{W}<em>{g e n}\left[\mathbf{s}</em>{j} ; \mathbf{c}<em>{j}\right]+\mathbf{b}</em>{g e n}\right)$</p>
<ul>
<li><p>本文还引入了copy机制</p>
<p>  ​                        $p_{j}=\lambda_{j} \cdot p_{g e n}+\left(1-\lambda_{j}\right) \cdot \sum_{i=1}^{|\mathbf{x}|} \alpha_{i j}$</p>
<p>  其中，</p>
<p>  ​                   $\lambda_{j}=\operatorname{sigmoid}\left(\mathbf{W}<em>{\lambda}\left[\mathbf{u}</em>{j} ; \mathbf{s}<em>{j} ; \mathbf{c}</em>{j} ; \theta\right]+\mathbf{b}_{\lambda}\right)$</p>
</li>
</ul>
<h2 id="Jointly-Learning-Topics-and-Keyphrases"><a href="#Jointly-Learning-Topics-and-Keyphrases" class="headerlink" title="Jointly Learning Topics and Keyphrases"></a>Jointly Learning Topics and Keyphrases</h2><ul>
<li>对于NTM，损失函数可定义为<br>  $$<br>  \mathcal{L}<em>{N T M}=D</em>{K L}(p(\mathbf{z}) | q(\mathbf{z} \mid \mathbf{x}))-\mathbb{E}_{q(\mathbf{z} \mid \mathbf{x})}[p(\mathbf{x} \mid \mathbf{z})]<br>  $$</li>
</ul>
<p>​        其中，</p>
<p>​                    第一项是KL散度损失(Kullback-Leibler divergence loss，也称相对熵)</p>
<p>​                    第二项是重构$w_i$的损失</p>
<blockquote>
<p>  <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xueqiuqiu/articles/7604390.html">https://www.cnblogs.com/xueqiuqiu/articles/7604390.html</a></p>
</blockquote>
<ul>
<li>对于关键词生成模型，损失函数定义为：<br>  $$<br>  \mathcal{L}<em>{K G}=-\sum</em>{n=1}^{N} \log \left(\operatorname{Pr}\left(\mathbf{y}<em>{n} \mid \mathbf{x}</em>{n}, \theta_{n}\right)\right)<br>  $$</li>
</ul>
<p>​             是一个生成模型常用的负对数损失函数。</p>
<ul>
<li>整个模型的损失函数：<br>  $$<br>  \mathcal{L}=\mathcal{L}<em>{N T M}+\gamma \cdot \mathcal{L}</em>{K G}<br>  $$</li>
</ul>
<p>​          $\gamma$是超参数</p>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><ul>
<li>  数据集</li>
</ul>
<p>​       three datasets collected from English and Chinese social media platform</p>
<img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117000857936.png" alt="image-20201117000857936" style="zoom: 50%;" />



<p>对于推文和微博，把hashtag当做是关键短语，如：</p>
<img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117001955609.png" alt="image-20201117001955609" style="zoom:50%;" />

<p>对于StackExchange,把作者打的标签作为关键短语，如：</p>
<img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117002236574.png" alt="image-20201117002236574" style="zoom:50%;" />



<ul>
<li><p>结果</p>
<p>  <img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117002859381.png" alt="image-20201117002859381"></p>
</li>
</ul>
<ul>
<li><p>Ablation Study</p>
<p>  <img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117004006325.png" alt="image-20201117004006325"></p>
</li>
</ul>
<p>被引调研</p>
<p>一篇关键词抽取综述</p>
<p>一篇Visual storytelling</p>
<p>一篇 Online Conversations 的引用  To help online users find what to quote in the discussions they are involved in   ACL2020</p>
<p>一篇关键词提取  只是用了这篇文章的数据集  基于词频的图构建</p>
<p>一篇关于social emotion  只是用了处理数据方式    ACL2020</p>
<p>一篇summarization    用了topic words matirix 来做legal dommain的summarization</p>
<p>一篇是在微博上提取主题   用的是repost构建图 用随机游走</p>
<p>一篇IEEE ACCESS      sentence  simplication</p>
<p>一篇关键词抽取  将PKE和AKE分开 PKE指导AKE  只是在introduction中提到这篇论文</p>
<p>一篇 Text Summarization   直接利用了NTM</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/11/16/Topic-Aware%20Neural%20Keyphrase%20Generation%20for%20Social%20Media%20Language/" data-id="ckjsjg4vz0000bj10gtpc1mk5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/01/11/KG-task01/">KG_task01</a>
          </li>
        
          <li>
            <a href="/2020/12/02/Heterogeneous%20Graph%20Neural%20Networks%20for%20Extractive%20Document%20Summarization/">(no title)</a>
          </li>
        
          <li>
            <a href="/2020/11/16/Topic-Aware%20Neural%20Keyphrase%20Generation%20for%20Social%20Media%20Language/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>