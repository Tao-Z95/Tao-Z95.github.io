<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="title:Topic-Aware Neural Keyphrase Generation for Social Media Languagedate:2020-12-27 14:43:12tags:note   Introduction现有的seq2seq模型对于处理经过精心编辑过的文章(科学文章)很有效 社交媒体中的文本：不正式，口语化 data sparsity：文本中的噪声比较大，而有用">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2020/11/16/Topic-Aware%20Neural%20Keyphrase%20Generation%20for%20Social%20Media%20Language/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="title:Topic-Aware Neural Keyphrase Generation for Social Media Languagedate:2020-12-27 14:43:12tags:note   Introduction现有的seq2seq模型对于处理经过精心编辑过的文章(科学文章)很有效 社交媒体中的文本：不正式，口语化 data sparsity：文本中的噪声比较大，而有用">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201116095902456.png">
<meta property="og:image" content="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201116160207666.png">
<meta property="og:image" content="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117000857936.png">
<meta property="og:image" content="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117001955609.png">
<meta property="og:image" content="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117002236574.png">
<meta property="og:image" content="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117002859381.png">
<meta property="og:image" content="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117004006325.png">
<meta property="article:published_time" content="2020-11-16T01:57:00.074Z">
<meta property="article:modified_time" content="2021-01-11T08:37:26.782Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201116095902456.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Topic-Aware Neural Keyphrase Generation for Social Media Language" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/11/16/Topic-Aware%20Neural%20Keyphrase%20Generation%20for%20Social%20Media%20Language/" class="article-date">
  <time datetime="2020-11-16T01:57:00.074Z" itemprop="datePublished">2020-11-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>title:Topic-Aware Neural Keyphrase Generation for Social Media Language<br>date:2020-12-27 14:43:12<br>tags:note</p>
<hr>
<p><img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201116095902456.png" alt="image-20201116095902456"></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>现有的seq2seq模型对于处理经过精心编辑过的文章(科学文章)很有效</p>
<p>社交媒体中的文本：不正式，口语化</p>
<p>data sparsity：文本中的噪声比较大，而有用信息比较少</p>
<p>本文提出的关键短语生成模型主要应用于处理这类社交媒体文本。</p>
<p><strong>intuition</strong>： topic words — naturally indicative of keyphrases</p>
<h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul>
<li><p> Leverag latent topic to enrich useful features</p>
<p>  提出了 一种新的基于主题感知的神经关键词生成模型。利用corpus-level latent topic来解决data sparsity问题，且这种方式不需要外部数据</p>
</li>
<li><p>taking advantage of the recent advance of neural topic model</p>
<p>  将神经主题模型引入端到端的关键短语生成中，并且一起训练</p>
</li>
<li><p>  experiment on three newly constructed so- cial media datasets</p>
</li>
</ul>
<p>​       在Twitter、 StackExchange、Weibo三个数据集上进行了实验，结果都比其他没有用到潜在主题的模型效果好。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><blockquote>
<p>  [Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, and Yu Chi. 2017. Deep keyphrase generation. In <em>Proceedings of Associa- tion for Computational Linguistics</em>.]</p>
</blockquote>
<blockquote>
<p>  [Jichuan Zeng, Jing Li, Yan Song, Cuiyun Gao, Michael R. Lyu, and Irwin King. 2018. Topic mem- ory networks for short text classification. In <em>Pro- ceedings of Empirical Methods in Natural Language Processing</em>.]</p>
</blockquote>
<p>本文提出的方法是结合了主题感知的关键短语生成，其模型框架如下图所示：</p>
<img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201116160207666.png" alt="image-20201116160207666" style="zoom: 50%;" />



<p>其中绿色的部分表示主题模型（topic model），上面的两部分整体上表示生成模型（generation model）<br>所要处理的语料库记为$\left{\mathbf{x}<em>{1}, \mathbf{x}</em>{2},\ldots,\mathbf{x}_{|C|}\right}$ ，$|C|$表示语料库中推文的数量，我们将每篇推文$\mathbf{x}$都处理成两种表示：</p>
<ul>
<li><p>一种是基于词袋模型的向量表示$\mathbf{x}_{bow}$，作为神经主题模型的输入。</p>
<p>  ​      其中，$\mathbf{x}_{bow}$是$V$维向量，$V$表示词汇表大小</p>
</li>
<li><p>  一种是词典索引序列向量的表示$\mathbf{x}_{seq}$,作为关键短语生成模型的输入</p>
</li>
</ul>
<h2 id="Neural-Topic-Model"><a href="#Neural-Topic-Model" class="headerlink" title="Neural Topic Model"></a>Neural Topic Model</h2><p>NTM整体上基于变分自编码器(Variational Auto-Encoder,VAE)的架构，</p>
<p>本文中的Neural topic Model就是用神经网络的方式来拟合文章的主题分布，它的任务就是对$\mathbf{x}<em>{bow}$进行重构得到$\mathbf{x}^{’}</em>{bow}，优化目标就是让两者尽可能接近。把中间过程得到的隐表示$\mathbf{z}$(表示$\mathbf{x}$的主题)参与生成模型Decoder的生成过程。</p>
<h3 id="BoW-Encoder"><a href="#BoW-Encoder" class="headerlink" title="BoW Encoder"></a>BoW Encoder</h3><p>这里BoW 编码器是用来生成这篇文章的主题的隐表示$\mathbf{z}$，其中 $\mathbf{z} \in \mathbb{R}^{K}$，$K$表示主题的数量，假设文章的主题的隐表示是连续的，且服从正态分布的,那只要根据$\mu$和$\sigma$就能生成服从正态分布的随机数$\mathbf{z}$。<br>$$<br>\mathbf{z} \sim \mathcal{N}\left(\mu, \sigma^{\mathbf{2}}\right)<br>$$<br>那么encoder的作用就是确定正态分布的两个参数$\mu$和$\sigma$。<br>$$<br>\mu=f_{\mu}\left(f_{e}\left(\mathbf{x}<em>{b o w}\right)\right), \log \sigma=f</em>{\sigma}\left(f_{e}\left(\mathbf{x}_{b o w}\right)\right)<br>$$</p>
<p>公式中的$f_{*}(·)$是以ReLU作为激活函数的感知机层。</p>
<blockquote>
<p>  [Jichuan Zeng, Jing Li, Yan Song, Cuiyun Gao, Michael R. Lyu, and Irwin King. 2018. Topic memory networks for short text classification. In <em>Pro- ceedings of Empirical Methods in Natural Language Processing</em>.]</p>
</blockquote>
<h3 id="BoW-Decoder"><a href="#BoW-Decoder" class="headerlink" title="BoW Decoder"></a>BoW Decoder</h3><p>对于语料库中的每篇推文，都有一个$K$维的topic mixture的概率分布θ：</p>
<p>$$<br>\theta=\operatorname{softmax}\left(f_{\theta}(\mathbf{z})\right)<br>$$<br>对于文章中的每个词$w\in\mathbf{x}$<br>$$<br>w \sim \operatorname{softmax}\left(f_{\phi}(\theta)\right)<br>$$<br>其中$f_{*}(·)$也是以ReLU作为激活函数的感知机层，那么$f_{\phi}(·)$的参数矩阵作为topic-word的分布，即每个topic对应的词的分布：<br>$$<br>\left(\phi_{1}, \phi_{2}, \ldots, \phi_{K}\right)<br>$$</p>
<h2 id="Neural-Keyphrase-Generation-Model"><a href="#Neural-Keyphrase-Generation-Model" class="headerlink" title="Neural Keyphrase Generation Model"></a>Neural Keyphrase Generation Model</h2><p>本文的Keyphrase Generation Model还是一个seq2seq的的模型，但是在decoder的输入端加入了文章的latent topic representation。</p>
<h3 id="Sequence-Encoder"><a href="#Sequence-Encoder" class="headerlink" title="Sequence Encoder"></a>Sequence Encoder</h3><p>采用了Bi-GRU来进行encode,<br>$$<br>\overrightarrow{\mathbf{h}}<em>{i}=f</em>{G R U}\left(\nu_{i}, \mathbf{h}_{i-1}\right)<br>$$</p>
<p>$$<br>\overleftarrow{\mathbf{h}}<em>{i}=f</em>{G R U}\left(\nu_{i}, \mathbf{h}_{i-1}\right)<br>$$</p>
<p>其中,</p>
<p>​                  $\nu_{i}$表示输入序列中词$w_i$的embedding</p>
<p>最后，</p>
<p>​                   得到${\mathbf{h}}<em>{i}=[\overrightarrow{\mathbf{h}}</em>{i};\overleftarrow{\mathbf{h}}_{i}]$  作为$w_i$的hidden state </p>
<p>​                   矩阵$\mathbf{M}=\left\langle\mathbf{h}<em>{1}, \mathbf{h}</em>{2}, \ldots, \mathbf{h}_{|\mathbf{x}|}\right\rangle$  用于做attention</p>
<h3 id="Topic-Aware-Sequence-Decoder"><a href="#Topic-Aware-Sequence-Decoder" class="headerlink" title="Topic-Aware Sequence Decoder"></a>Topic-Aware Sequence Decoder</h3><p>根据Sequence Encoder和Neural topic model的输出，生成每个关键短语的概率可表示为：<br>$$<br>\operatorname{Pr}(\mathbf{y} \mid \mathbf{x})=\prod_{j=1}^{|\mathbf{y}|} \operatorname{Pr}\left(y_{j} \mid \mathbf{y}_{&lt;j}, \mathbf{M}, \theta\right)<br>$$</p>
<ul>
<li>本文的sequence decoder采用了一个单向GRU，第$j$个时间步的hidden state：</li>
</ul>
<p>$$<br>\mathbf{s}<em>{j}=f</em>{G R U}\left(\left[\mathbf{u}<em>{j} ; \theta\right], \mathbf{s}</em>{j-1}\right)<br>$$</p>
<p>其中,</p>
<p>​                         $\mathbf{u}_{j}$是decoder第j步输出的embedding</p>
<ul>
<li><p>本文的decoder还采用了attention机制</p>
<p>   attention的权重：<br>  $$<br>  \alpha_{i j}=\frac{\exp \left(f_{\alpha}\left(\mathbf{h}<em>{i}, \mathbf{s}</em>{j}, \theta\right)\right)}{\sum_{i^{\prime}=1}^{|\mathbf{x}|} \exp \left(f_{\alpha}\left(\mathbf{h}<em>{i^{\prime}}, \mathbf{s}</em>{j}, \theta\right)\right)}<br>  $$</p>
</li>
</ul>
<p>​          其中，</p>
<p>​                                       $f_{\alpha}\left(\mathbf{h}<em>{i}, \mathbf{s}</em>{j}, \theta\right)=\mathbf{v}<em>{\alpha}^{T} \tanh \left(\mathbf{W}</em>{\alpha}\left[\mathbf{h}<em>{i} ; \mathbf{s}</em>{j} ; \theta\right]+\mathbf{b}_{\alpha}\right)$</p>
<p>​           得到topic sensitive context vector $\mathbf{c}_{j}$ ：</p>
<p>​                                                  $\mathbf{c}<em>{j}=\sum</em>{i=1}^{|\mathbf{x}|} \alpha_{i j} \mathbf{h}_{i}$</p>
<p>​           </p>
<p>​        最终得到第j个词在词汇表上的生成概率：</p>
<p>​      $p_{g e n}=\operatorname{softmax}\left(\mathbf{W}<em>{g e n}\left[\mathbf{s}</em>{j} ; \mathbf{c}<em>{j}\right]+\mathbf{b}</em>{g e n}\right)$</p>
<ul>
<li><p>本文还引入了copy机制</p>
<p>  ​                        $p_{j}=\lambda_{j} \cdot p_{g e n}+\left(1-\lambda_{j}\right) \cdot \sum_{i=1}^{|\mathbf{x}|} \alpha_{i j}$</p>
<p>  其中，</p>
<p>  ​                   $\lambda_{j}=\operatorname{sigmoid}\left(\mathbf{W}<em>{\lambda}\left[\mathbf{u}</em>{j} ; \mathbf{s}<em>{j} ; \mathbf{c}</em>{j} ; \theta\right]+\mathbf{b}_{\lambda}\right)$</p>
</li>
</ul>
<h2 id="Jointly-Learning-Topics-and-Keyphrases"><a href="#Jointly-Learning-Topics-and-Keyphrases" class="headerlink" title="Jointly Learning Topics and Keyphrases"></a>Jointly Learning Topics and Keyphrases</h2><ul>
<li>对于NTM，损失函数可定义为<br>  $$<br>  \mathcal{L}<em>{N T M}=D</em>{K L}(p(\mathbf{z}) | q(\mathbf{z} \mid \mathbf{x}))-\mathbb{E}_{q(\mathbf{z} \mid \mathbf{x})}[p(\mathbf{x} \mid \mathbf{z})]<br>  $$</li>
</ul>
<p>​        其中，</p>
<p>​                    第一项是KL散度损失(Kullback-Leibler divergence loss，也称相对熵)</p>
<p>​                    第二项是重构$w_i$的损失</p>
<blockquote>
<p>  <a target="_blank" rel="noopener" href="https://www.cnblogs.com/xueqiuqiu/articles/7604390.html">https://www.cnblogs.com/xueqiuqiu/articles/7604390.html</a></p>
</blockquote>
<ul>
<li>对于关键词生成模型，损失函数定义为：<br>  $$<br>  \mathcal{L}<em>{K G}=-\sum</em>{n=1}^{N} \log \left(\operatorname{Pr}\left(\mathbf{y}<em>{n} \mid \mathbf{x}</em>{n}, \theta_{n}\right)\right)<br>  $$</li>
</ul>
<p>​             是一个生成模型常用的负对数损失函数。</p>
<ul>
<li>整个模型的损失函数：<br>  $$<br>  \mathcal{L}=\mathcal{L}<em>{N T M}+\gamma \cdot \mathcal{L}</em>{K G}<br>  $$</li>
</ul>
<p>​          $\gamma$是超参数</p>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><ul>
<li>  数据集</li>
</ul>
<p>​       three datasets collected from English and Chinese social media platform</p>
<img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117000857936.png" alt="image-20201117000857936" style="zoom: 50%;" />



<p>对于推文和微博，把hashtag当做是关键短语，如：</p>
<img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117001955609.png" alt="image-20201117001955609" style="zoom:50%;" />

<p>对于StackExchange,把作者打的标签作为关键短语，如：</p>
<img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117002236574.png" alt="image-20201117002236574" style="zoom:50%;" />



<ul>
<li><p>结果</p>
<p>  <img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117002859381.png" alt="image-20201117002859381"></p>
</li>
</ul>
<ul>
<li><p>Ablation Study</p>
<p>  <img src="https://typora-upic-1304199839.cos.ap-shanghai.myqcloud.com/upload/image-20201117004006325.png" alt="image-20201117004006325"></p>
</li>
</ul>
<p>被引调研</p>
<p>一篇关键词抽取综述</p>
<p>一篇Visual storytelling</p>
<p>一篇 Online Conversations 的引用  To help online users find what to quote in the discussions they are involved in   ACL2020</p>
<p>一篇关键词提取  只是用了这篇文章的数据集  基于词频的图构建</p>
<p>一篇关于social emotion  只是用了处理数据方式    ACL2020</p>
<p>一篇summarization    用了topic words matirix 来做legal dommain的summarization</p>
<p>一篇是在微博上提取主题   用的是repost构建图 用随机游走</p>
<p>一篇IEEE ACCESS      sentence  simplication</p>
<p>一篇关键词抽取  将PKE和AKE分开 PKE指导AKE  只是在introduction中提到这篇论文</p>
<p>一篇 Text Summarization   直接利用了NTM</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/11/16/Topic-Aware%20Neural%20Keyphrase%20Generation%20for%20Social%20Media%20Language/" data-id="ckjsjg4vz0000bj10gtpc1mk5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/12/02/Heterogeneous%20Graph%20Neural%20Networks%20for%20Extractive%20Document%20Summarization/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/12/02/Heterogeneous%20Graph%20Neural%20Networks%20for%20Extractive%20Document%20Summarization/">(no title)</a>
          </li>
        
          <li>
            <a href="/2020/11/16/Topic-Aware%20Neural%20Keyphrase%20Generation%20for%20Social%20Media%20Language/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>